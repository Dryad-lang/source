# ğŸ”¤ Sistema de Lexer

O Lexer (analisador lÃ©xico) Ã© o primeiro componente da pipeline do Dryad. Ele converte cÃ³digo fonte em uma sequÃªncia de tokens que podem ser processados pelo parser.

## ğŸ¯ Objetivo e Responsabilidades

O Lexer Ã© responsÃ¡vel por:
- **TokenizaÃ§Ã£o:** Converter texto em tokens estruturados
- **Reconhecimento de padrÃµes:** Identificar palavras-chave, operadores, literais
- **Filtragem:** Ignorar espaÃ§os em branco e comentÃ¡rios (quando implementados)
- **DetecÃ§Ã£o de erros lÃ©xicos:** Caracteres invÃ¡lidos, strings nÃ£o fechadas, etc.

## ğŸ—ï¸ Arquitetura

### Estrutura de Arquivos
```
src/lexer/
â”œâ”€â”€ mod.rs          # MÃ³dulo principal e re-exports
â”œâ”€â”€ token.rs        # DefiniÃ§Ã£o de todos os tokens
â””â”€â”€ tokenizer.rs    # ImplementaÃ§Ã£o do lexer
```

### Fluxo de Funcionamento
```
CÃ³digo Fonte â†’ AnÃ¡lise Caractere por Caractere â†’ Token â†’ Token â†’ ... â†’ EOF
```

## ğŸ“‹ Tipos de Tokens

### DefiniÃ§Ã£o Completa (`src/lexer/token.rs`)
```rust
#[derive(Debug, Clone, PartialEq)]
pub enum Token {
    // Palavras-chave
    Let, Fun, If, Else, For, While, Return, Export, Use,
    
    // Literais
    Identifier(String),  // nomes de variÃ¡veis/funÃ§Ãµes
    Number(f64),         // nÃºmeros (42, 3.14)
    String(String),      // strings ("hello")
    
    // Operadores aritmÃ©ticos
    Plus, Minus, Star, Slash,  // + - * /
    
    // Operadores de comparaÃ§Ã£o
    Equal, EqualEqual,         // = ==
    Bang, BangEqual,           // ! !=
    Less, LessEqual,           // < <=
    Greater, GreaterEqual,     // > >=
    
    // SÃ­mbolos estruturais
    LParen, RParen,      // ( )
    LBrace, RBrace,      // { }
    Comma, Semicolon,    // , ;
    
    // Marcador de fim
    Eof,
}
```

### Categorias de Tokens

#### **1. Palavras-chave**
```rust
"let"    â†’ Token::Let
"fun"    â†’ Token::Fun
"if"     â†’ Token::If
"else"   â†’ Token::Else
"for"    â†’ Token::For
"while"  â†’ Token::While
"return" â†’ Token::Return
"export" â†’ Token::Export
"use"    â†’ Token::Use
```

#### **2. Identificadores**
```rust
"variable"   â†’ Token::Identifier("variable".to_string())
"myFunction" â†’ Token::Identifier("myFunction".to_string())
"_private"   â†’ Token::Identifier("_private".to_string())
"snake_case" â†’ Token::Identifier("snake_case".to_string())
```

**Regras:**
- Deve comeÃ§ar com letra ou underscore
- Pode conter letras, nÃºmeros e underscores
- Case-sensitive

#### **3. NÃºmeros**
```rust
"42"     â†’ Token::Number(42.0)
"3.14"   â†’ Token::Number(3.14)
"0.5"    â†’ Token::Number(0.5)
"999"    â†’ Token::Number(999.0)
```

**Formatos suportados:**
- Inteiros: `42`, `0`, `999`
- Decimais: `3.14`, `0.5`, `123.456`

#### **4. Strings**
```rust
"\"hello\""        â†’ Token::String("hello".to_string())
"\"world test\""   â†’ Token::String("world test".to_string())
"\"\""             â†’ Token::String("".to_string())
```

## ğŸ”§ ImplementaÃ§Ã£o do Lexer

### Estrutura Principal (`src/lexer/tokenizer.rs`)
```rust
pub struct Lexer {
    input: Vec<char>,    // CÃ³digo fonte como array de caracteres
    current: usize,      // PosiÃ§Ã£o atual no array
}

impl Lexer {
    pub fn new(input: &str) -> Self {
        Self {
            input: input.chars().collect(),
            current: 0,
        }
    }
    
    pub fn next_token(&mut self) -> Token {
        // MÃ©todo principal que retorna o prÃ³ximo token
    }
}
```

### MÃ©todos Auxiliares

#### **NavegaÃ§Ã£o no CÃ³digo**
```rust
fn is_at_end(&self) -> bool {
    self.current >= self.input.len()
}

fn advance(&mut self) -> char {
    let ch = self.input[self.current];
    self.current += 1;
    ch
}

fn peek(&self) -> Option<char> {
    self.input.get(self.current).copied()
}

fn match_char(&mut self, expected: char) -> bool {
    if self.peek() == Some(expected) {
        self.advance();
        true
    } else {
        false
    }
}
```

#### **AnÃ¡lise de ConteÃºdo**
```rust
fn skip_whitespace(&mut self) {
    while let Some(&ch) = self.input.get(self.current) {
        if ch.is_whitespace() {
            self.advance();
        } else {
            break;
        }
    }
}

fn read_number(&mut self, first: char) -> Token {
    let mut number = first.to_string();
    
    // Ler parte inteira
    while let Some(&ch) = self.input.get(self.current) {
        if ch.is_ascii_digit() {
            number.push(self.advance());
        } else {
            break;
        }
    }
    
    // Verificar se hÃ¡ parte decimal
    if self.peek() == Some('.') && 
       self.input.get(self.current + 1).map_or(false, |c| c.is_ascii_digit()) {
        number.push(self.advance()); // '.'
        
        while let Some(&ch) = self.input.get(self.current) {
            if ch.is_ascii_digit() {
                number.push(self.advance());
            } else {
                break;
            }
        }
    }
    
    Token::Number(number.parse().unwrap())
}

fn read_string(&mut self) -> Token {
    let mut string = String::new();
    
    while let Some(&ch) = self.input.get(self.current) {
        if ch == '"' {
            self.advance(); // Fechar aspas
            break;
        }
        string.push(self.advance());
    }
    
    Token::String(string)
}

fn read_identifier(&mut self, first: char) -> Token {
    let mut ident = first.to_string();
    
    while let Some(&ch) = self.input.get(self.current) {
        if ch.is_alphanumeric() || ch == '_' {
            ident.push(self.advance());
        } else {
            break;
        }
    }
    
    // Verificar se Ã© palavra-chave
    match ident.as_str() {
        "let" => Token::Let,
        "fun" => Token::Fun,
        "if" => Token::If,
        "else" => Token::Else,
        "for" => Token::For,
        "while" => Token::While,
        "return" => Token::Return,
        "export" => Token::Export,
        "use" => Token::Use,
        _ => Token::Identifier(ident),
    }
}
```

### MÃ©todo Principal - `next_token()`
```rust
pub fn next_token(&mut self) -> Token {
    self.skip_whitespace();
    
    if self.is_at_end() {
        return Token::Eof;
    }
    
    let c = self.advance();
    
    match c {
        '+' => Token::Plus,
        '-' => Token::Minus,
        '*' => Token::Star,
        '/' => Token::Slash,
        '(' => Token::LParen,
        ')' => Token::RParen,
        '{' => Token::LBrace,
        '}' => Token::RBrace,
        ',' => Token::Comma,
        ';' => Token::Semicolon,
        
        '=' => {
            if self.match_char('=') {
                Token::EqualEqual
            } else {
                Token::Equal
            }
        }
        
        '!' => {
            if self.match_char('=') {
                Token::BangEqual
            } else {
                Token::Bang
            }
        }
        
        '<' => {
            if self.match_char('=') {
                Token::LessEqual
            } else {
                Token::Less
            }
        }
        
        '>' => {
            if self.match_char('=') {
                Token::GreaterEqual
            } else {
                Token::Greater
            }
        }
        
        '"' => self.read_string(),
        
        c if c.is_ascii_digit() => self.read_number(c),
        
        c if Self::is_identifier_start(c) => self.read_identifier(c),
        
        _ => {
            // Caractere nÃ£o reconhecido - poderia gerar erro
            Token::Eof
        }
    }
}
```

## ğŸ“Š Exemplo de TokenizaÃ§Ã£o

### Entrada:
```javascript
let x = 42 + 8;
if (x > 10) {
    x = x * 2;
}
```

### Processo passo a passo:

1. **`let`** â†’ `Token::Let`
2. **` `** â†’ (ignorado - whitespace)
3. **`x`** â†’ `Token::Identifier("x".to_string())`
4. **` `** â†’ (ignorado)
5. **`=`** â†’ `Token::Equal`
6. **` `** â†’ (ignorado)
7. **`42`** â†’ `Token::Number(42.0)` (chama `read_number`)
8. **` `** â†’ (ignorado)
9. **`+`** â†’ `Token::Plus`
10. **` `** â†’ (ignorado)
11. **`8`** â†’ `Token::Number(8.0)`
12. **`;`** â†’ `Token::Semicolon`
... e assim por diante

### SaÃ­da Final:
```rust
[
    Token::Let,
    Token::Identifier("x".to_string()),
    Token::Equal,
    Token::Number(42.0),
    Token::Plus,
    Token::Number(8.0),
    Token::Semicolon,
    Token::If,
    Token::LParen,
    Token::Identifier("x".to_string()),
    Token::Greater,
    Token::Number(10.0),
    Token::RParen,
    Token::LBrace,
    Token::Identifier("x".to_string()),
    Token::Equal,
    Token::Identifier("x".to_string()),
    Token::Star,
    Token::Number(2.0),
    Token::Semicolon,
    Token::RBrace,
    Token::Eof,
]
```

## ğŸ” Tratamento de Casos Especiais

### **Operadores Compostos**
```rust
'=' seguido de '=' â†’ Token::EqualEqual
'!' seguido de '=' â†’ Token::BangEqual
'<' seguido de '=' â†’ Token::LessEqual
'>' seguido de '=' â†’ Token::GreaterEqual
```

### **NÃºmeros Decimais**
```rust
"3.14" â†’ LÃª '3', verifica '.', lÃª '14' â†’ Token::Number(3.14)
"42"   â†’ LÃª '42', nÃ£o encontra '.', â†’ Token::Number(42.0)
```

### **Identificadores vs Palavras-chave**
```rust
"let"    â†’ Reconhece como palavra-chave â†’ Token::Let
"letter" â†’ Reconhece como identificador â†’ Token::Identifier("letter")
```

## ğŸš€ Estendendo o Lexer

### **Adicionando Novos Operadores**

1. **Definir o token:**
```rust
// Em src/lexer/token.rs
pub enum Token {
    // ... tokens existentes ...
    Percent,  // %
}
```

2. **Implementar reconhecimento:**
```rust
// Em src/lexer/tokenizer.rs, no mÃ©todo next_token()
match c {
    // ... casos existentes ...
    '%' => Token::Percent,
}
```

### **Adicionando Novas Palavras-chave**

1. **Definir o token:**
```rust
pub enum Token {
    // ... tokens existentes ...
    Match,  // match
}
```

2. **Atualizar reconhecimento:**
```rust
// Em read_identifier()
match ident.as_str() {
    // ... casos existentes ...
    "match" => Token::Match,
    _ => Token::Identifier(ident),
}
```

### **Suporte a ComentÃ¡rios (futuro)**
```rust
'/' seguido de '/' â†’ Ignora atÃ© fim da linha
'/' seguido de '*' â†’ Ignora atÃ© encontrar '*/'
```

## âœ… Estado Atual vs Futuras Melhorias

### **Implementado:**
- âœ… Tokens bÃ¡sicos (palavras-chave, operadores, literais)
- âœ… NÃºmeros inteiros e decimais
- âœ… Strings bÃ¡sicas
- âœ… Identificadores
- âœ… Operadores compostos

### **Planejado:**
- ğŸ”„ ComentÃ¡rios (`//` e `/* */`)
- ğŸ”„ Escape sequences em strings (`\n`, `\"`, etc.)
- ğŸ”„ NÃºmeros em outras bases (hexadecimal, binÃ¡rio)
- ğŸ”„ Melhor tratamento de erros lÃ©xicos
- ğŸ”„ InformaÃ§Ãµes de localizaÃ§Ã£o (linha, coluna)

O Lexer atual Ã© robusto e suficiente para as funcionalidades implementadas do Dryad, fornecendo uma base sÃ³lida para extensÃµes futuras.
